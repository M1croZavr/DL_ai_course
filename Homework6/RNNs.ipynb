{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==0.4.1\n",
      "ERROR: No matching distribution found for torch==0.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\BobPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\BobPc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'VERB', 'PRT', 'ADP', 'NUM', 'NOUN', 'ADV', 'CONJ', '.', 'ADJ', 'DET', 'X', 'PRON'}\n"
     ]
    }
   ],
   "source": [
    "# Get indexes for words in train_data\n",
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind for ind, word in enumerate(words, 1)}\n",
    "# In word_to_index dictionary <pad> has index 0\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind for ind, tag in enumerate(tags, 1)}\n",
    "# In tag_to_index dictionary <pad> has index 0\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    # If word not in words dict then index 0\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    '''\n",
    "    This function-generator yields batches of X_batch and y_batch.\n",
    "    Batch consists of sentences.\n",
    "    In case all elements in batch must be regular sized, we pad by 0\n",
    "    every sentence that less than maximum length sentence of batch.'''\n",
    "    # Full X and Y of dataset\n",
    "    X, y = data\n",
    "    # Number of sentences\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Making indices of sentences and shuffling them\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Iterates over batches\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        # If the end of batch\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        # Extacting indices of a current batch\n",
    "        batch_indices = indices[start:end]\n",
    "        # Maximum length of a sentence of batch\n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        # Initializing batches (rows-words of a sentence, column-sentence of batch)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        # Iterating over batch indices\n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            # Assign a column exact words of a sentence, if len(sentece) < max(len(sentence)) zeros left\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "        # yield batch of X(sentences of words) and Y(POSes of words)    \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3) (24, 3)\n",
      "[[ 8838. 37929.  5805.]\n",
      " [29209. 39911. 41109.]\n",
      " [ 7839.  6924. 37929.]\n",
      " [10885. 27385. 35898.]\n",
      " [35368.   562. 41109.]\n",
      " [14140.  6262. 27194.]\n",
      " [    0.  6137. 33331.]\n",
      " [    0.  4505. 34924.]\n",
      " [    0. 31860. 32457.]\n",
      " [    0. 14140. 12732.]\n",
      " [    0.     0. 37377.]\n",
      " [    0.     0.  9666.]\n",
      " [    0.     0.  7287.]\n",
      " [    0.     0. 40383.]\n",
      " [    0.     0. 24902.]\n",
      " [    0.     0.  9406.]\n",
      " [    0.     0. 27652.]\n",
      " [    0.     0. 33742.]\n",
      " [    0.     0.  1975.]\n",
      " [    0.     0. 13392.]\n",
      " [    0.     0.  6475.]\n",
      " [    0.     0. 23012.]\n",
      " [    0.     0. 32900.]\n",
      " [    0.     0. 14140.]]\n"
     ]
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 3))\n",
    "\n",
    "print(X_batch.shape, y_batch.shape)\n",
    "print(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        # Hidden state dim of LSTM\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # This layer embeds batch of sentences\n",
    "        self.word_embedder = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        # LSTM takes embeddings dim and hiddens (dim of hidden state) as parameters\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
    "        # FC layer (hidden state to softmax), hidden state dim as input parameter\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # By default hidden state and cell state are provided as zeros tensors in input of LSTM in torch\n",
    "        # h0 = torch.zeros(1 * self.layers_count, inputs.shape[1], self.hidden_dim, requires_grad=True).cuda()\n",
    "        # c0 = torch.zeros(1 * self.layers_count, inputs.shape[1], self.hidden_dim, requires_grad=True).cuda()\n",
    "        # Embeddings inputs are indices of sentences in generated batch\n",
    "        embeddings = self.word_embedder(inputs)\n",
    "        # lstm inputs are input tensor of shape (Seq_len, batch_size, embed_size), (h and c are zeros tensors by default)\n",
    "        # nn.Embedding returns correct dimension output\n",
    "        # Output of LSTM contains output dim(Seq, Batch, Hidden_state) these are h_t for each t, also h_n, c_n\n",
    "        lstm_out, (hn, cn) = self.lstm(embeddings)\n",
    "        # FC layer to classification\n",
    "        output = self.fc(lstm_out.view(-1, self.hidden_dim))    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1, 2, 3, 4],\n",
    "                   [5, 6, 7, 8]],\n",
    "                  [[10, 12, 13, 14],\n",
    "                   [15, 16, 17, 18]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [10, 12, 13, 14],\n",
       "        [15, 16, 17, 18]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(logits, gt):\n",
    "    return torch.sum(torch.argmax(logits, dim=1) == gt.flatten()) / (gt.shape[0] * gt.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0555555559694767\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "#<calc accuracy>\n",
    "print('Accuracy: {}'.format(calc_accuracy(logits, y_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 13])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5879, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# <calc loss>\n",
    "loss = criterion(logits, y_batch.flatten())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8838, 37929,  5805, 29209, 39911, 41109,  7839,  6924, 37929, 10885,\n",
       "        27385, 35898, 35368,   562, 41109, 14140,  6262, 27194,     0,  6137,\n",
       "        33331,     0,  4505, 34924,     0, 31860, 32457,     0, 14140, 12732,\n",
       "            0,     0, 37377,     0,     0,  9666,     0,     0,  7287,     0,\n",
       "            0, 40383,     0,     0, 24902,     0,     0,  9406,     0,     0,\n",
       "        27652,     0,     0, 33742,     0,     0,  1975,     0,     0, 13392,\n",
       "            0,     0,  6475,     0,     0, 23012,     0,     0, 32900,     0,\n",
       "            0, 14140])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits_, X_batch_, y_batch_flatten_):\n",
    "    # Applying mask of zeros of pads and making torch.argmax()\n",
    "    masked = torch.argmax(logits_ * X_batch_.flatten().to(torch.bool)[:, None], dim=1)\n",
    "    correct_cnt = torch.sum(masked == y_batch_flatten_)\n",
    "    all_cnt = len(y_batch_flatten_)\n",
    "    zeros = all_cnt - torch.count_nonzero(y_batch_flatten_)\n",
    "    return correct_cnt - zeros, all_cnt - zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train) # mode = True/False\n",
    "    # batches_count = len(X) / batch_size\n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                y_batch_flatten = LongTensor(y_batch.flatten())\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits, y_batch_flatten) # <calc loss>\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                cur_correct_count, cur_sum_count =  compute_accuracy(logits, X_batch, y_batch_flatten)\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32, val_data=None, val_batch_size=None):\n",
    "    \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.30537, Accuracy = 73.12%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 55.79it/s]\n",
      "[1 / 50]   Val: Loss = 0.09350, Accuracy = 85.01%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 43.64it/s]\n",
      "[2 / 50] Train: Loss = 0.10014, Accuracy = 90.15%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.81it/s]\n",
      "[2 / 50]   Val: Loss = 0.07753, Accuracy = 89.39%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[3 / 50] Train: Loss = 0.06759, Accuracy = 93.28%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 59.48it/s]\n",
      "[3 / 50]   Val: Loss = 0.06571, Accuracy = 91.12%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n",
      "[4 / 50] Train: Loss = 0.05115, Accuracy = 94.88%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.98it/s]\n",
      "[4 / 50]   Val: Loss = 0.06430, Accuracy = 92.08%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[5 / 50] Train: Loss = 0.04062, Accuracy = 95.88%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.94it/s]\n",
      "[5 / 50]   Val: Loss = 0.05880, Accuracy = 92.59%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.46it/s]\n",
      "[6 / 50] Train: Loss = 0.03289, Accuracy = 96.62%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 59.04it/s]\n",
      "[6 / 50]   Val: Loss = 0.06485, Accuracy = 92.89%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 51.61it/s]\n",
      "[7 / 50] Train: Loss = 0.02728, Accuracy = 97.19%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 59.09it/s]\n",
      "[7 / 50]   Val: Loss = 0.06067, Accuracy = 93.11%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 48.98it/s]\n",
      "[8 / 50] Train: Loss = 0.02244, Accuracy = 97.66%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.84it/s]\n",
      "[8 / 50]   Val: Loss = 0.07095, Accuracy = 93.13%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 51.60it/s]\n",
      "[9 / 50] Train: Loss = 0.01874, Accuracy = 98.05%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.18it/s]\n",
      "[9 / 50]   Val: Loss = 0.06317, Accuracy = 93.29%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.46it/s]\n",
      "[10 / 50] Train: Loss = 0.01542, Accuracy = 98.40%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.39it/s]\n",
      "[10 / 50]   Val: Loss = 0.06693, Accuracy = 93.26%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.63it/s]\n",
      "[11 / 50] Train: Loss = 0.01280, Accuracy = 98.69%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.70it/s]\n",
      "[11 / 50]   Val: Loss = 0.07048, Accuracy = 93.31%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.80it/s]\n",
      "[12 / 50] Train: Loss = 0.01069, Accuracy = 98.93%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.46it/s]\n",
      "[12 / 50]   Val: Loss = 0.07424, Accuracy = 93.28%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[13 / 50] Train: Loss = 0.00867, Accuracy = 99.15%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.64it/s]\n",
      "[13 / 50]   Val: Loss = 0.07180, Accuracy = 93.28%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.34it/s]\n",
      "[14 / 50] Train: Loss = 0.00697, Accuracy = 99.33%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.09it/s]\n",
      "[14 / 50]   Val: Loss = 0.08125, Accuracy = 93.25%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.63it/s]\n",
      "[15 / 50] Train: Loss = 0.00570, Accuracy = 99.46%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.37it/s]\n",
      "[15 / 50]   Val: Loss = 0.08160, Accuracy = 93.26%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.64it/s]\n",
      "[16 / 50] Train: Loss = 0.00465, Accuracy = 99.57%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.20it/s]\n",
      "[16 / 50]   Val: Loss = 0.09281, Accuracy = 93.23%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.40it/s]\n",
      "[17 / 50] Train: Loss = 0.00381, Accuracy = 99.66%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.49it/s]\n",
      "[17 / 50]   Val: Loss = 0.09295, Accuracy = 93.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.07it/s]\n",
      "[18 / 50] Train: Loss = 0.00325, Accuracy = 99.70%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.73it/s]\n",
      "[18 / 50]   Val: Loss = 0.09506, Accuracy = 93.17%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.81it/s]\n",
      "[19 / 50] Train: Loss = 0.00266, Accuracy = 99.76%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.38it/s]\n",
      "[19 / 50]   Val: Loss = 0.09359, Accuracy = 93.19%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.81it/s]\n",
      "[20 / 50] Train: Loss = 0.00247, Accuracy = 99.78%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.24it/s]\n",
      "[20 / 50]   Val: Loss = 0.10048, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.82it/s]\n",
      "[21 / 50] Train: Loss = 0.00221, Accuracy = 99.79%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.99it/s]\n",
      "[21 / 50]   Val: Loss = 0.10871, Accuracy = 93.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 41.95it/s]\n",
      "[22 / 50] Train: Loss = 0.00234, Accuracy = 99.77%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.99it/s]\n",
      "[22 / 50]   Val: Loss = 0.11079, Accuracy = 93.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 41.41it/s]\n",
      "[23 / 50] Train: Loss = 0.00189, Accuracy = 99.81%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.45it/s]\n",
      "[23 / 50]   Val: Loss = 0.11677, Accuracy = 93.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[24 / 50] Train: Loss = 0.00176, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.49it/s]\n",
      "[24 / 50]   Val: Loss = 0.10891, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.83it/s]\n",
      "[25 / 50] Train: Loss = 0.00179, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.26it/s]\n",
      "[25 / 50]   Val: Loss = 0.11811, Accuracy = 93.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.00it/s]\n",
      "[26 / 50] Train: Loss = 0.00172, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.14it/s]\n",
      "[26 / 50]   Val: Loss = 0.11456, Accuracy = 93.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 46.11it/s]\n",
      "[27 / 50] Train: Loss = 0.00162, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.21it/s]\n",
      "[27 / 50]   Val: Loss = 0.11860, Accuracy = 93.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.63it/s]\n",
      "[28 / 50] Train: Loss = 0.00159, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.87it/s]\n",
      "[28 / 50]   Val: Loss = 0.12246, Accuracy = 93.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.00it/s]\n",
      "[29 / 50] Train: Loss = 0.00235, Accuracy = 99.75%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.55it/s]\n",
      "[29 / 50]   Val: Loss = 0.12399, Accuracy = 93.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.40it/s]\n",
      "[30 / 50] Train: Loss = 0.00157, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.20it/s]\n",
      "[30 / 50]   Val: Loss = 0.13031, Accuracy = 93.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[31 / 50] Train: Loss = 0.00142, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.76it/s]\n",
      "[31 / 50]   Val: Loss = 0.13302, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.82it/s]\n",
      "[32 / 50] Train: Loss = 0.00140, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.24it/s]\n",
      "[32 / 50]   Val: Loss = 0.13628, Accuracy = 93.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.20it/s]\n",
      "[33 / 50] Train: Loss = 0.00135, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.80it/s]\n",
      "[33 / 50]   Val: Loss = 0.14259, Accuracy = 93.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.60it/s]\n",
      "[34 / 50] Train: Loss = 0.00136, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.16it/s]\n",
      "[34 / 50]   Val: Loss = 0.13903, Accuracy = 93.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.63it/s]\n",
      "[35 / 50] Train: Loss = 0.00186, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.10it/s]\n",
      "[35 / 50]   Val: Loss = 0.13793, Accuracy = 93.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[36 / 50] Train: Loss = 0.00225, Accuracy = 99.75%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.19it/s]\n",
      "[36 / 50]   Val: Loss = 0.14005, Accuracy = 93.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n",
      "[37 / 50] Train: Loss = 0.00144, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.79it/s]\n",
      "[37 / 50]   Val: Loss = 0.14726, Accuracy = 93.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[38 / 50] Train: Loss = 0.00127, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.49it/s]\n",
      "[38 / 50]   Val: Loss = 0.14092, Accuracy = 93.13%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.46it/s]\n",
      "[39 / 50] Train: Loss = 0.00126, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.54it/s]\n",
      "[39 / 50]   Val: Loss = 0.13983, Accuracy = 93.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 45.31it/s]\n",
      "[40 / 50] Train: Loss = 0.00128, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.64it/s]\n",
      "[40 / 50]   Val: Loss = 0.14742, Accuracy = 93.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.45it/s]\n",
      "[41 / 50] Train: Loss = 0.00130, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.07it/s]\n",
      "[41 / 50]   Val: Loss = 0.14781, Accuracy = 93.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.82it/s]\n",
      "[42 / 50] Train: Loss = 0.00228, Accuracy = 99.74%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.59it/s]\n",
      "[42 / 50]   Val: Loss = 0.14321, Accuracy = 93.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.79it/s]\n",
      "[43 / 50] Train: Loss = 0.00144, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.99it/s]\n",
      "[43 / 50]   Val: Loss = 0.14737, Accuracy = 93.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.26it/s]\n",
      "[44 / 50] Train: Loss = 0.00127, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.82it/s]\n",
      "[44 / 50]   Val: Loss = 0.14379, Accuracy = 93.19%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.71it/s]\n",
      "[45 / 50] Train: Loss = 0.00124, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.62it/s]\n",
      "[45 / 50]   Val: Loss = 0.15262, Accuracy = 93.13%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[46 / 50] Train: Loss = 0.00124, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.83it/s]\n",
      "[46 / 50]   Val: Loss = 0.15344, Accuracy = 93.20%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[47 / 50] Train: Loss = 0.00125, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.09it/s]\n",
      "[47 / 50]   Val: Loss = 0.14907, Accuracy = 93.18%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.26it/s]\n",
      "[48 / 50] Train: Loss = 0.00128, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.78it/s]\n",
      "[48 / 50]   Val: Loss = 0.15036, Accuracy = 93.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.07it/s]\n",
      "[49 / 50] Train: Loss = 0.00139, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.05it/s]\n",
      "[49 / 50]   Val: Loss = 0.16685, Accuracy = 93.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.20it/s]\n",
      "[50 / 50] Train: Loss = 0.00236, Accuracy = 99.73%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.64it/s]\n",
      "[50 / 50]   Val: Loss = 0.16526, Accuracy = 93.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.68656, Accuracy = 78.40%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.25it/s]\n",
      "[1 / 50]   Val: Loss = 0.35818, Accuracy = 86.59%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 52.02it/s]\n",
      "[2 / 50] Train: Loss = 0.27279, Accuracy = 91.00%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.79it/s]\n",
      "[2 / 50]   Val: Loss = 0.23387, Accuracy = 90.56%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[3 / 50] Train: Loss = 0.18375, Accuracy = 93.94%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 59.10it/s]\n",
      "[3 / 50]   Val: Loss = 0.18429, Accuracy = 92.07%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 48.71it/s]\n",
      "[4 / 50] Train: Loss = 0.13719, Accuracy = 95.42%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 58.36it/s]\n",
      "[4 / 50]   Val: Loss = 0.16457, Accuracy = 92.75%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 51.00it/s]\n",
      "[5 / 50] Train: Loss = 0.10683, Accuracy = 96.41%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 59.25it/s]\n",
      "[5 / 50]   Val: Loss = 0.15489, Accuracy = 93.16%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 49.63it/s]\n",
      "[6 / 50] Train: Loss = 0.08504, Accuracy = 97.11%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 57.47it/s]\n",
      "[6 / 50]   Val: Loss = 0.15113, Accuracy = 93.32%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 42.50it/s]\n",
      "[7 / 50] Train: Loss = 0.06830, Accuracy = 97.70%: 100%|█████████████████████████████| 572/572 [00:09<00:00, 60.11it/s]\n",
      "[7 / 50]   Val: Loss = 0.14759, Accuracy = 93.53%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 53.52it/s]\n",
      "[8 / 50] Train: Loss = 0.05539, Accuracy = 98.14%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 56.97it/s]\n",
      "[8 / 50]   Val: Loss = 0.15189, Accuracy = 93.53%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 48.34it/s]\n",
      "[9 / 50] Train: Loss = 0.04520, Accuracy = 98.49%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 57.19it/s]\n",
      "[9 / 50]   Val: Loss = 0.15772, Accuracy = 93.57%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.81it/s]\n",
      "[10 / 50] Train: Loss = 0.03707, Accuracy = 98.78%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.39it/s]\n",
      "[10 / 50]   Val: Loss = 0.16201, Accuracy = 93.59%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[11 / 50] Train: Loss = 0.03033, Accuracy = 99.00%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.29it/s]\n",
      "[11 / 50]   Val: Loss = 0.17177, Accuracy = 93.63%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.60it/s]\n",
      "[12 / 50] Train: Loss = 0.02468, Accuracy = 99.21%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.01it/s]\n",
      "[12 / 50]   Val: Loss = 0.17963, Accuracy = 93.57%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.52it/s]\n",
      "[13 / 50] Train: Loss = 0.02023, Accuracy = 99.37%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.10it/s]\n",
      "[13 / 50]   Val: Loss = 0.19284, Accuracy = 93.53%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 52.23it/s]\n",
      "[14 / 50] Train: Loss = 0.01624, Accuracy = 99.51%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.04it/s]\n",
      "[14 / 50]   Val: Loss = 0.19885, Accuracy = 93.53%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.71it/s]\n",
      "[15 / 50] Train: Loss = 0.01339, Accuracy = 99.60%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.04it/s]\n",
      "[15 / 50]   Val: Loss = 0.20923, Accuracy = 93.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.60it/s]\n",
      "[16 / 50] Train: Loss = 0.01113, Accuracy = 99.67%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.64it/s]\n",
      "[16 / 50]   Val: Loss = 0.22072, Accuracy = 93.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.81it/s]\n",
      "[17 / 50] Train: Loss = 0.00954, Accuracy = 99.72%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.01it/s]\n",
      "[17 / 50]   Val: Loss = 0.22887, Accuracy = 93.45%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.07it/s]\n",
      "[18 / 50] Train: Loss = 0.00801, Accuracy = 99.76%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.88it/s]\n",
      "[18 / 50]   Val: Loss = 0.23962, Accuracy = 93.45%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.71it/s]\n",
      "[19 / 50] Train: Loss = 0.00712, Accuracy = 99.79%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.73it/s]\n",
      "[19 / 50]   Val: Loss = 0.24913, Accuracy = 93.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.90it/s]\n",
      "[20 / 50] Train: Loss = 0.00639, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.72it/s]\n",
      "[20 / 50]   Val: Loss = 0.25197, Accuracy = 93.38%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 52.65it/s]\n",
      "[21 / 50] Train: Loss = 0.00596, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.21it/s]\n",
      "[21 / 50]   Val: Loss = 0.26176, Accuracy = 93.37%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.52it/s]\n",
      "[22 / 50] Train: Loss = 0.00565, Accuracy = 99.81%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.93it/s]\n",
      "[22 / 50]   Val: Loss = 0.28165, Accuracy = 93.38%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.63it/s]\n",
      "[23 / 50] Train: Loss = 0.00609, Accuracy = 99.80%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.63it/s]\n",
      "[23 / 50]   Val: Loss = 0.27579, Accuracy = 93.37%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 47.99it/s]\n",
      "[24 / 50] Train: Loss = 0.00531, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.93it/s]\n",
      "[24 / 50]   Val: Loss = 0.27653, Accuracy = 93.36%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n",
      "[25 / 50] Train: Loss = 0.00518, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.63it/s]\n",
      "[25 / 50]   Val: Loss = 0.28135, Accuracy = 93.40%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.40it/s]\n",
      "[26 / 50] Train: Loss = 0.00508, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.30it/s]\n",
      "[26 / 50]   Val: Loss = 0.29230, Accuracy = 93.31%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.71it/s]\n",
      "[27 / 50] Train: Loss = 0.00472, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.05it/s]\n",
      "[27 / 50]   Val: Loss = 0.29425, Accuracy = 93.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.89it/s]\n",
      "[28 / 50] Train: Loss = 0.00508, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:10<00:00, 57.17it/s]\n",
      "[28 / 50]   Val: Loss = 0.29618, Accuracy = 93.31%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n",
      "[29 / 50] Train: Loss = 0.00492, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.83it/s]\n",
      "[29 / 50]   Val: Loss = 0.29452, Accuracy = 93.34%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 52.23it/s]\n",
      "[30 / 50] Train: Loss = 0.00466, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.99it/s]\n",
      "[30 / 50]   Val: Loss = 0.29915, Accuracy = 93.37%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.52it/s]\n",
      "[31 / 50] Train: Loss = 0.00465, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.74it/s]\n",
      "[31 / 50]   Val: Loss = 0.29917, Accuracy = 93.33%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.34it/s]\n",
      "[32 / 50] Train: Loss = 0.00455, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.24it/s]\n",
      "[32 / 50]   Val: Loss = 0.30646, Accuracy = 93.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 41.68it/s]\n",
      "[33 / 50] Train: Loss = 0.00425, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.94it/s]\n",
      "[33 / 50]   Val: Loss = 0.30755, Accuracy = 93.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 44.54it/s]\n",
      "[34 / 50] Train: Loss = 0.00428, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.66it/s]\n",
      "[34 / 50]   Val: Loss = 0.30750, Accuracy = 93.35%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 53.30it/s]\n",
      "[35 / 50] Train: Loss = 0.00500, Accuracy = 99.82%: 100%|████████████████████████████| 572/572 [00:09<00:00, 60.81it/s]\n",
      "[35 / 50]   Val: Loss = 0.30401, Accuracy = 93.40%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.60it/s]\n",
      "[36 / 50] Train: Loss = 0.00497, Accuracy = 99.81%: 100%|████████████████████████████| 572/572 [00:10<00:00, 57.19it/s]\n",
      "[36 / 50]   Val: Loss = 0.30870, Accuracy = 93.39%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.45it/s]\n",
      "[37 / 50] Train: Loss = 0.00426, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 57.98it/s]\n",
      "[37 / 50]   Val: Loss = 0.30734, Accuracy = 93.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.16it/s]\n",
      "[38 / 50] Train: Loss = 0.00417, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.22it/s]\n",
      "[38 / 50]   Val: Loss = 0.30772, Accuracy = 93.43%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.40it/s]\n",
      "[39 / 50] Train: Loss = 0.00417, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.06it/s]\n",
      "[39 / 50]   Val: Loss = 0.31343, Accuracy = 93.42%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.59it/s]\n",
      "[40 / 50] Train: Loss = 0.00399, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 59.19it/s]\n",
      "[40 / 50]   Val: Loss = 0.31559, Accuracy = 93.36%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.60it/s]\n",
      "[41 / 50] Train: Loss = 0.00392, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.67it/s]\n",
      "[41 / 50]   Val: Loss = 0.31262, Accuracy = 93.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.20it/s]\n",
      "[42 / 50] Train: Loss = 0.00387, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.59it/s]\n",
      "[42 / 50]   Val: Loss = 0.31140, Accuracy = 93.46%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.00it/s]\n",
      "[43 / 50] Train: Loss = 0.00390, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.83it/s]\n",
      "[43 / 50]   Val: Loss = 0.31538, Accuracy = 93.43%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.45it/s]\n",
      "[44 / 50] Train: Loss = 0.00684, Accuracy = 99.75%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.82it/s]\n",
      "[44 / 50]   Val: Loss = 0.30866, Accuracy = 93.41%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 51.00it/s]\n",
      "[45 / 50] Train: Loss = 0.00442, Accuracy = 99.83%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.62it/s]\n",
      "[45 / 50]   Val: Loss = 0.31259, Accuracy = 93.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.26it/s]\n",
      "[46 / 50] Train: Loss = 0.00372, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.34it/s]\n",
      "[46 / 50]   Val: Loss = 0.31242, Accuracy = 93.46%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[47 / 50] Train: Loss = 0.00365, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.63it/s]\n",
      "[47 / 50]   Val: Loss = 0.31549, Accuracy = 93.46%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.82it/s]\n",
      "[48 / 50] Train: Loss = 0.00372, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.80it/s]\n",
      "[48 / 50]   Val: Loss = 0.31637, Accuracy = 93.47%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n",
      "[49 / 50] Train: Loss = 0.00373, Accuracy = 99.84%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.74it/s]\n",
      "[49 / 50]   Val: Loss = 0.31713, Accuracy = 93.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 52.02it/s]\n",
      "[50 / 50] Train: Loss = 0.00380, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:09<00:00, 58.43it/s]\n",
      "[50 / 50]   Val: Loss = 0.31377, Accuracy = 93.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Test Loss = 0.31103, Accuracy = 93.61%: 100%|██████████████████████████████████████| 224/224 [00:01<00:00, 157.91it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = do_epoch(model, criterion, (X_test, y_test), 64, name='Test') # Better than baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        # Hidden state dim of LSTM\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # This layer embeds batch of sentences\n",
    "        self.word_embedder = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        # LSTM takes embeddings dim and hiddens (dim of hidden state) as parameters\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_coun, bidirectional=True)\n",
    "        # FC layer (hidden state to softmax) (lstm output -> POS), hidden state dim as input parameter\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # By default hidden state and cell state are provided as zeros tensors in input of LSTM in torch\n",
    "        # Embeddings inputs are indices of sentences\n",
    "        embeddings = self.word_embedder(inputs)\n",
    "        # lstm inputs are input tensor of shape (Seq_len, batch_size, embed_size), (h and c are zeros tensors by default)\n",
    "        # Output of LSTM contains output dim(Seq, Batch, Hidden_state) these are h_t for each t, h_n, c_n\n",
    "        lstm_out, (hn, cn) = self.lstm(embeddings)\n",
    "        # FC layer to classification\n",
    "        output = self.fc(lstm_out.view(-1, self.hidden_dim * 2))    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BobPc\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45441, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.hidden_state_size = lstm_hidden_dim\n",
    "        self.embedder = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.embedder(inputs)\n",
    "        # hn and cn are final states of each batch\n",
    "        output, (hn, cn) = self.lstm(output)\n",
    "        # output remains order as embeddings\n",
    "        # Right order to use with flatten y_batch\n",
    "        output = self.fc(output.view(-1, self.hidden_state_size))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.75122, Accuracy = 78.31%: 100%|████████████████████████████| 572/572 [00:04<00:00, 117.05it/s]\n",
      "[1 / 50]   Val: Loss = 0.36852, Accuracy = 87.80%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 66.69it/s]\n",
      "[2 / 50] Train: Loss = 0.28489, Accuracy = 91.51%: 100%|████████████████████████████| 572/572 [00:04<00:00, 119.56it/s]\n",
      "[2 / 50]   Val: Loss = 0.25338, Accuracy = 90.79%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 71.85it/s]\n",
      "[3 / 50] Train: Loss = 0.21013, Accuracy = 93.49%: 100%|████████████████████████████| 572/572 [00:04<00:00, 119.79it/s]\n",
      "[3 / 50]   Val: Loss = 0.20745, Accuracy = 91.98%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 72.25it/s]\n",
      "[4 / 50] Train: Loss = 0.17455, Accuracy = 94.46%: 100%|████████████████████████████| 572/572 [00:04<00:00, 118.28it/s]\n",
      "[4 / 50]   Val: Loss = 0.18250, Accuracy = 92.74%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 68.44it/s]\n",
      "[5 / 50] Train: Loss = 0.15355, Accuracy = 95.06%: 100%|████████████████████████████| 572/572 [00:04<00:00, 115.10it/s]\n",
      "[5 / 50]   Val: Loss = 0.16663, Accuracy = 93.15%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 69.92it/s]\n",
      "[6 / 50] Train: Loss = 0.13939, Accuracy = 95.45%: 100%|████████████████████████████| 572/572 [00:04<00:00, 121.50it/s]\n",
      "[6 / 50]   Val: Loss = 0.15773, Accuracy = 93.36%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 72.65it/s]\n",
      "[7 / 50] Train: Loss = 0.12947, Accuracy = 95.70%: 100%|████████████████████████████| 572/572 [00:04<00:00, 119.70it/s]\n",
      "[7 / 50]   Val: Loss = 0.15080, Accuracy = 93.49%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 72.25it/s]\n",
      "[8 / 50] Train: Loss = 0.12200, Accuracy = 95.91%: 100%|████████████████████████████| 572/572 [00:04<00:00, 119.49it/s]\n",
      "[8 / 50]   Val: Loss = 0.14521, Accuracy = 93.68%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 69.92it/s]\n",
      "[9 / 50] Train: Loss = 0.11628, Accuracy = 96.07%: 100%|████████████████████████████| 572/572 [00:05<00:00, 113.09it/s]\n",
      "[9 / 50]   Val: Loss = 0.14142, Accuracy = 93.76%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 69.54it/s]\n",
      "[10 / 50] Train: Loss = 0.11158, Accuracy = 96.18%: 100%|███████████████████████████| 572/572 [00:04<00:00, 120.37it/s]\n",
      "[10 / 50]   Val: Loss = 0.13886, Accuracy = 93.84%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 74.31it/s]\n",
      "[11 / 50] Train: Loss = 0.10751, Accuracy = 96.29%: 100%|███████████████████████████| 572/572 [00:04<00:00, 123.09it/s]\n",
      "[11 / 50]   Val: Loss = 0.13696, Accuracy = 93.91%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 67.38it/s]\n",
      "[12 / 50] Train: Loss = 0.10389, Accuracy = 96.41%: 100%|███████████████████████████| 572/572 [00:04<00:00, 122.35it/s]\n",
      "[12 / 50]   Val: Loss = 0.13444, Accuracy = 93.95%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.92it/s]\n",
      "[13 / 50] Train: Loss = 0.10109, Accuracy = 96.50%: 100%|███████████████████████████| 572/572 [00:04<00:00, 122.30it/s]\n",
      "[13 / 50]   Val: Loss = 0.13338, Accuracy = 94.00%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.45it/s]\n",
      "[14 / 50] Train: Loss = 0.09872, Accuracy = 96.55%: 100%|███████████████████████████| 572/572 [00:04<00:00, 123.68it/s]\n",
      "[14 / 50]   Val: Loss = 0.13206, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.92it/s]\n",
      "[15 / 50] Train: Loss = 0.09618, Accuracy = 96.63%: 100%|███████████████████████████| 572/572 [00:04<00:00, 123.51it/s]\n",
      "[15 / 50]   Val: Loss = 0.13229, Accuracy = 94.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.45it/s]\n",
      "[16 / 50] Train: Loss = 0.09439, Accuracy = 96.68%: 100%|███████████████████████████| 572/572 [00:04<00:00, 123.82it/s]\n",
      "[16 / 50]   Val: Loss = 0.13127, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.85it/s]\n",
      "[17 / 50] Train: Loss = 0.09209, Accuracy = 96.75%: 100%|███████████████████████████| 572/572 [00:04<00:00, 124.58it/s]\n",
      "[17 / 50]   Val: Loss = 0.13035, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.45it/s]\n",
      "[18 / 50] Train: Loss = 0.09071, Accuracy = 96.80%: 100%|███████████████████████████| 572/572 [00:04<00:00, 117.73it/s]\n",
      "[18 / 50]   Val: Loss = 0.12950, Accuracy = 94.13%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.54it/s]\n",
      "[19 / 50] Train: Loss = 0.08893, Accuracy = 96.85%: 100%|███████████████████████████| 572/572 [00:05<00:00, 114.15it/s]\n",
      "[19 / 50]   Val: Loss = 0.12882, Accuracy = 94.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.18it/s]\n",
      "[20 / 50] Train: Loss = 0.08743, Accuracy = 96.89%: 100%|███████████████████████████| 572/572 [00:04<00:00, 114.70it/s]\n",
      "[20 / 50]   Val: Loss = 0.13120, Accuracy = 94.03%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.17it/s]\n",
      "[21 / 50] Train: Loss = 0.08602, Accuracy = 96.94%: 100%|███████████████████████████| 572/572 [00:04<00:00, 114.94it/s]\n",
      "[21 / 50]   Val: Loss = 0.12962, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.68it/s]\n",
      "[22 / 50] Train: Loss = 0.08455, Accuracy = 96.99%: 100%|███████████████████████████| 572/572 [00:05<00:00, 114.39it/s]\n",
      "[22 / 50]   Val: Loss = 0.12996, Accuracy = 94.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 72.65it/s]\n",
      "[23 / 50] Train: Loss = 0.08339, Accuracy = 97.02%: 100%|███████████████████████████| 572/572 [00:04<00:00, 115.67it/s]\n",
      "[23 / 50]   Val: Loss = 0.12999, Accuracy = 94.16%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.54it/s]\n",
      "[24 / 50] Train: Loss = 0.08210, Accuracy = 97.05%: 100%|███████████████████████████| 572/572 [00:05<00:00, 111.15it/s]\n",
      "[24 / 50]   Val: Loss = 0.12967, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.34it/s]\n",
      "[25 / 50] Train: Loss = 0.08116, Accuracy = 97.10%: 100%|███████████████████████████| 572/572 [00:05<00:00, 113.70it/s]\n",
      "[25 / 50]   Val: Loss = 0.12925, Accuracy = 94.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 50.21it/s]\n",
      "[26 / 50] Train: Loss = 0.07986, Accuracy = 97.14%: 100%|███████████████████████████| 572/572 [00:05<00:00, 113.89it/s]\n",
      "[26 / 50]   Val: Loss = 0.13025, Accuracy = 94.16%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 58.05it/s]\n",
      "[27 / 50] Train: Loss = 0.07890, Accuracy = 97.19%: 100%|███████████████████████████| 572/572 [00:05<00:00, 111.48it/s]\n",
      "[27 / 50]   Val: Loss = 0.13012, Accuracy = 94.13%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 66.69it/s]\n",
      "[28 / 50] Train: Loss = 0.07798, Accuracy = 97.21%: 100%|███████████████████████████| 572/572 [00:05<00:00, 113.92it/s]\n",
      "[28 / 50]   Val: Loss = 0.13045, Accuracy = 94.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 65.02it/s]\n",
      "[29 / 50] Train: Loss = 0.07694, Accuracy = 97.24%: 100%|███████████████████████████| 572/572 [00:05<00:00, 111.96it/s]\n",
      "[29 / 50]   Val: Loss = 0.13130, Accuracy = 94.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 65.02it/s]\n",
      "[30 / 50] Train: Loss = 0.07615, Accuracy = 97.27%: 100%|███████████████████████████| 572/572 [00:05<00:00, 113.10it/s]\n",
      "[30 / 50]   Val: Loss = 0.13112, Accuracy = 94.19%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 57.54it/s]\n",
      "[31 / 50] Train: Loss = 0.07531, Accuracy = 97.31%: 100%|███████████████████████████| 572/572 [00:04<00:00, 120.73it/s]\n",
      "[31 / 50]   Val: Loss = 0.13239, Accuracy = 94.11%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.53it/s]\n",
      "[32 / 50] Train: Loss = 0.07439, Accuracy = 97.32%: 100%|███████████████████████████| 572/572 [00:04<00:00, 125.45it/s]\n",
      "[32 / 50]   Val: Loss = 0.13174, Accuracy = 94.16%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 72.65it/s]\n",
      "[33 / 50] Train: Loss = 0.07384, Accuracy = 97.36%: 100%|███████████████████████████| 572/572 [00:04<00:00, 125.29it/s]\n",
      "[33 / 50]   Val: Loss = 0.13211, Accuracy = 94.16%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.68it/s]\n",
      "[34 / 50] Train: Loss = 0.07288, Accuracy = 97.39%: 100%|███████████████████████████| 572/572 [00:04<00:00, 123.20it/s]\n",
      "[34 / 50]   Val: Loss = 0.13269, Accuracy = 94.08%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 69.92it/s]\n",
      "[35 / 50] Train: Loss = 0.07210, Accuracy = 97.40%: 100%|███████████████████████████| 572/572 [00:05<00:00, 113.53it/s]\n",
      "[35 / 50]   Val: Loss = 0.13290, Accuracy = 94.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 62.22it/s]\n",
      "[36 / 50] Train: Loss = 0.07148, Accuracy = 97.42%: 100%|███████████████████████████| 572/572 [00:05<00:00, 102.94it/s]\n",
      "[36 / 50]   Val: Loss = 0.13377, Accuracy = 94.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 62.52it/s]\n",
      "[37 / 50] Train: Loss = 0.07065, Accuracy = 97.45%: 100%|███████████████████████████| 572/572 [00:05<00:00, 110.28it/s]\n",
      "[37 / 50]   Val: Loss = 0.13410, Accuracy = 94.13%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 68.81it/s]\n",
      "[38 / 50] Train: Loss = 0.07015, Accuracy = 97.47%: 100%|███████████████████████████| 572/572 [00:04<00:00, 117.74it/s]\n",
      "[38 / 50]   Val: Loss = 0.13695, Accuracy = 94.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.67it/s]\n",
      "[39 / 50] Train: Loss = 0.06927, Accuracy = 97.52%: 100%|███████████████████████████| 572/572 [00:04<00:00, 116.25it/s]\n",
      "[39 / 50]   Val: Loss = 0.13533, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.06it/s]\n",
      "[40 / 50] Train: Loss = 0.06877, Accuracy = 97.53%: 100%|███████████████████████████| 572/572 [00:04<00:00, 120.25it/s]\n",
      "[40 / 50]   Val: Loss = 0.13587, Accuracy = 94.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.45it/s]\n",
      "[41 / 50] Train: Loss = 0.06797, Accuracy = 97.56%: 100%|███████████████████████████| 572/572 [00:04<00:00, 119.16it/s]\n",
      "[41 / 50]   Val: Loss = 0.13533, Accuracy = 94.09%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 65.68it/s]\n",
      "[42 / 50] Train: Loss = 0.06736, Accuracy = 97.57%: 100%|███████████████████████████| 572/572 [00:04<00:00, 117.75it/s]\n",
      "[42 / 50]   Val: Loss = 0.13689, Accuracy = 94.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.29it/s]\n",
      "[43 / 50] Train: Loss = 0.06684, Accuracy = 97.60%: 100%|███████████████████████████| 572/572 [00:04<00:00, 116.19it/s]\n",
      "[43 / 50]   Val: Loss = 0.13703, Accuracy = 94.14%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 71.38it/s]\n",
      "[44 / 50] Train: Loss = 0.06634, Accuracy = 97.62%: 100%|███████████████████████████| 572/572 [00:04<00:00, 118.03it/s]\n",
      "[44 / 50]   Val: Loss = 0.13844, Accuracy = 94.07%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 68.81it/s]\n",
      "[45 / 50] Train: Loss = 0.06560, Accuracy = 97.64%: 100%|███████████████████████████| 572/572 [00:04<00:00, 117.82it/s]\n",
      "[45 / 50]   Val: Loss = 0.13906, Accuracy = 94.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 67.38it/s]\n",
      "[46 / 50] Train: Loss = 0.06524, Accuracy = 97.66%: 100%|███████████████████████████| 572/572 [00:04<00:00, 114.87it/s]\n",
      "[46 / 50]   Val: Loss = 0.13925, Accuracy = 94.05%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 68.08it/s]\n",
      "[47 / 50] Train: Loss = 0.06443, Accuracy = 97.68%: 100%|███████████████████████████| 572/572 [00:05<00:00, 112.86it/s]\n",
      "[47 / 50]   Val: Loss = 0.13971, Accuracy = 94.06%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 66.69it/s]\n",
      "[48 / 50] Train: Loss = 0.06409, Accuracy = 97.70%: 100%|███████████████████████████| 572/572 [00:04<00:00, 119.60it/s]\n",
      "[48 / 50]   Val: Loss = 0.14192, Accuracy = 94.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.29it/s]\n",
      "[49 / 50] Train: Loss = 0.06345, Accuracy = 97.74%: 100%|███████████████████████████| 572/572 [00:05<00:00, 114.20it/s]\n",
      "[49 / 50]   Val: Loss = 0.14191, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 70.29it/s]\n",
      "[50 / 50] Train: Loss = 0.06305, Accuracy = 97.74%: 100%|███████████████████████████| 572/572 [00:04<00:00, 114.53it/s]\n",
      "[50 / 50]   Val: Loss = 0.14167, Accuracy = 94.04%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 68.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Test Loss = 0.12527, Accuracy = 95.97%: 100%|██████████████████████████████████████| 112/112 [00:00<00:00, 148.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12526794629437582, tensor(0.9597, device='cuda:0'))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_epoch(model, criterion, (X_test, y_test), 128, name='Test') # Better than baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
